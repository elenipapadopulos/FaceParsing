# -*- coding: utf-8 -*-
"""Si_ricomincia (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IudnZIiHztWaM6nnB1GzJZUKQNBlRagk

# Import libraries and dataset
"""

!pip install -q datasets
# !pip install -q git+https://github.com/huggingface/transformers.git
!pip install -q albumentations
!pip install git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/pedropro/TACO
# %cd TACO

!pip install -r requirements.txt

# Commented out IPython magic to ensure Python compatibility.
## ---- CONTROLLA ALLA FINE SE CI SERVE TUTTO
from PIL import Image, ExifTags
import matplotlib.pyplot as plt
from pycocotools.coco import COCO
from datasets import load_dataset
# per fare i poligoni. ci serve ancora?
from matplotlib.patches import Polygon, Rectangle
from matplotlib.collections import PatchCollection
import colorsys
import random
import pylab
import requests
import torch
from urllib.request import urlopen
import albumentations as A
# %matplotlib inline
import json
import numpy as np
import pandas as pd
import seaborn as sns; sns.set()

from google.colab import files

uploaded = files.upload()

import pickle

# # Load
with open('dataset_taco.pkl', 'rb') as f:
    dataset_taco = pickle.load(f)

dataset_taco1 = load_dataset("RandyHuynh5815/TACO-Reformatted-Full")

"""**TACOset** ha il formato dataset. Da usare quando si vuole visualizzare un'immagine

e.g. img = TACOset[12]['image']
"""

TACOset = dataset_taco1['train'].select(range(300))

plt.imshow(TACOset[230]['image'])

dataset_taco['images'][230]

dataset_path = './data'
anns_file_path = dataset_path + '/' + 'annotations.json'

coco = COCO(anns_file_path)

"""# Preprocessing

## Label 60
"""

for c in dataset_taco['categories']:
  if c['id'] == 0:
    c['id'] = 60

for a in dataset_taco['annotations']:
  if a['category_id'] == 0:
    a['category_id'] = 60


dataset_taco['categories'].append({'supercategory': 'Background', 'id': 0, 'name': 'Background'})

"""## Color palette"""

def color_palette():
    """Color palette that maps each class to RGB values.

    This one is actually taken from ADE20k.
    """
    return [[120, 120, 120], [180, 120, 120], [6, 230, 230], [80, 50, 50],
            [4, 200, 3], [120, 120, 80], [140, 140, 140], [204, 5, 255],
            [230, 230, 230], [4, 250, 7], [224, 5, 255], [235, 255, 7],
            [150, 5, 61], [120, 120, 70], [8, 255, 51], [255, 6, 82],
            [143, 255, 140], [204, 255, 4], [255, 51, 7], [204, 70, 3],
            [0, 102, 200], [61, 230, 250], [255, 6, 51], [11, 102, 255],
            [255, 7, 71], [255, 9, 224], [9, 7, 230], [220, 220, 220],
            [255, 9, 92], [112, 9, 255], [8, 255, 214], [7, 255, 224],
            [255, 184, 6], [10, 255, 71], [255, 41, 10], [7, 255, 255],
            [224, 255, 8], [102, 8, 255], [255, 61, 6], [255, 194, 7],
            [255, 122, 8], [0, 255, 20], [255, 8, 41], [255, 5, 153],
            [6, 51, 255], [235, 12, 255], [160, 150, 20], [0, 163, 255],
            [140, 140, 140], [250, 10, 15], [20, 255, 0], [31, 255, 0],
            [255, 31, 0], [255, 224, 0], [153, 255, 0], [0, 0, 255],
            [255, 71, 0], [0, 235, 255], [0, 173, 255], [31, 0, 255],
            [11, 200, 200], [255, 82, 0], [0, 255, 245], [0, 61, 255],
            [0, 255, 112], [0, 255, 133], [255, 0, 0], [255, 163, 0],
            [255, 102, 0], [194, 255, 0], [0, 143, 255], [51, 255, 0],
            [0, 82, 255], [0, 255, 41], [0, 255, 173], [10, 0, 255],
            [173, 255, 0], [0, 255, 153], [255, 92, 0], [255, 0, 255],
            [255, 0, 245], [255, 0, 102], [255, 173, 0], [255, 0, 20],
            [255, 184, 184], [0, 31, 255], [0, 255, 61], [0, 71, 255],
            [255, 0, 204], [0, 255, 194], [0, 255, 82], [0, 10, 255],
            [0, 112, 255], [51, 0, 255], [0, 194, 255], [0, 122, 255],
            [0, 255, 163], [255, 153, 0], [0, 255, 10], [255, 112, 0],
            [143, 255, 0], [82, 0, 255], [163, 255, 0], [255, 235, 0],
            [8, 184, 170], [133, 0, 255], [0, 255, 92], [184, 0, 255],
            [255, 0, 31], [0, 184, 255], [0, 214, 255], [255, 0, 112],
            [92, 255, 0], [0, 224, 255], [112, 224, 255], [70, 184, 160],
            [163, 0, 255], [153, 0, 255], [71, 255, 0], [255, 0, 163],
            [255, 204, 0], [255, 0, 143], [0, 255, 235], [133, 255, 0],
            [255, 0, 235], [245, 0, 255], [255, 0, 122], [255, 245, 0],
            [10, 190, 212], [214, 255, 0], [0, 204, 255], [20, 0, 255],
            [255, 255, 0], [0, 153, 255], [0, 41, 255], [0, 255, 204],
            [41, 0, 255], [41, 255, 0], [173, 0, 255], [0, 245, 255],
            [71, 0, 255], [122, 0, 255], [0, 255, 184], [0, 92, 255],
            [184, 255, 0], [0, 133, 255], [255, 214, 0], [25, 194, 194],
            [102, 255, 0], [92, 0, 255]]

palette = color_palette()

"""## Mask generation"""

import torch

# Assicurati che la GPU sia disponibile
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

coco = COCO(anns_file_path)
indexes_to_remove = []

TACOset2 = []

for ind, img in enumerate(dataset_taco['images'][:300]):

  if img['id'] == 93:
    indexes_to_remove.append(ind)
    continue

  image = {}
  image['id'] = img['id']

  image['image'] = TACOset[img['id']]['image'] # rendi lista

  annIds = coco.getAnnIds(imgIds=img['id'], iscrowd=None)
  anns_sel = coco.loadAnns(annIds)

  binary_mask = []
  categories = []

  for i, ann in enumerate(anns_sel):
    binary_mask.append(coco.annToMask(ann))
    categories.append(ann['category_id'])

  image['binary_masks'] = torch.tensor(np.array(binary_mask)).to(device)
  image['label_classes'] = torch.tensor(np.array(categories)).to(device)

  TACOset2.append(image)

"""## Data visualization

"""

TACOset2[119]

id = 162
img = TACOset2[id]['image']
plt.imshow(img)

img = TACOset2[id]['binary_masks'][0].cpu()
plt.imshow(img)

"""## Train and test split"""

from sklearn.model_selection import train_test_split
import random


# # shuffle + split dataset
TACOset_train, TACOset_test = train_test_split(TACOset2, test_size=0.25, random_state = 2026)

import torchvision.transforms as transforms
import PIL.Image as Image

# Define the transformation
to_tensor = transforms.ToTensor()

# Open an image
image = Image.open('image.jpg')

# Apply the ToTensor transformation
tensor_image = to_tensor(image)

# Obtain Exif orientation tag code
# for orientation in ExifTags.TAGS.keys():
#     if ExifTags.TAGS[orientation] == 'Orientation':
#         break
import numpy as np
from torch.utils.data import Dataset
import requests
from PIL import Image
import matplotlib.pyplot as plt

import urllib.request
from urllib.request import urlopen


class ImageSegmentationDataset(Dataset):
    """Image segmentation dataset."""

    def __init__(self, dataset, transform):
        """
        Args:
            dataset
        """
        self.dataset = dataset
        self.transform = transform

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):

      for orientation in ExifTags.TAGS.keys():
        if ExifTags.TAGS[orientation] == 'Orientation':
            break

      I = self.dataset[idx]['image']

      # Load and process image metadata
      if I._getexif():
        exif = dict(I._getexif().items())
        # Rotate portrait and upside down images if necessary
        if orientation in exif:
            if exif[orientation] == 3:
                I = I.rotate(180,expand=True)
            if exif[orientation] == 6:
                I = I.rotate(270,expand=True)
            if exif[orientation] == 8:
                I = I.rotate(90,expand=True)

      original_image = np.array(I)
      original_binary_masks = np.array(self.dataset[idx]['binary_masks'].cpu())

      transformed = self.transform(image=original_image, mask=original_binary_masks.transpose(1,2,0))

      image, binary_masks = transformed['image'], transformed['mask']

      original_id = self.dataset[idx]['id']
      label_classes = self.dataset[idx]['label_classes'].cpu()

      # convert to C, H, W
      #image = image.transpose(2,0,1)
      # binary_masks = torch.tensor(binary_masks.transpose(2,0,1))

      return to_tensor(image), to_tensor(binary_masks), label_classes, original_id

torch.tensor(image.transpose(2,0,1))

import albumentations as A

ADE_MEAN = np.array([123.675, 116.280, 103.530]) / 255
ADE_STD = np.array([58.395, 57.120, 57.375]) / 255

train_transform = A.Compose([
    A.LongestMaxSize(max_size=1200),
    A.Resize(width=256, height=256),
    A.HorizontalFlip(p=0.5)],
    # A.Normalize(mean=ADE_MEAN, std=ADE_STD)],
    is_check_shapes = False
)

test_transform = A.Compose([
    A.Resize(width=256, height=256),
    A.Normalize(mean=ADE_MEAN, std=ADE_STD)],
    is_check_shapes = False

)

train_dataset = ImageSegmentationDataset(TACOset_train, transform=train_transform)
test_dataset = ImageSegmentationDataset(TACOset_test, transform=test_transform)

image, binary_masks, label_classes, original_id = train_dataset[45]

binary_masks

image.shape

plt.imshow(image[0].cpu())

binary_masks.shape

plt.imshow(binary_masks[0])

id2label = {}
for k in dataset_taco['categories']:
  id2label[k['id']] = k['name']

print(id2label)

from torch.utils.data import DataLoader

def collate_fn(batch):
    inputs = list(zip(*batch))
    # images = inputs[0]
    # segmentation_maps = inputs[1]
    # this function pads the inputs to the same size,
    # and creates a pixel mask
    # actually padding isn't required here since we are cropping
    '''
    batch = preprocessor(
        images,
        segmentation_maps=segmentation_maps,
        return_tensors="pt",
    )
    '''
    return {
        "pixel_values": torch.stack(inputs[0]),
        "mask_labels": list(inputs[1]),
        "class_labels": list(inputs[2])
    }


train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)
test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)

batch = next((iter(train_dataloader)))

batch['pixel_values']

"""# Train and test"""

pip install transformers

from transformers import MaskFormerForInstanceSegmentation

# Replace the head of the pre-trained model
model = MaskFormerForInstanceSegmentation.from_pretrained("facebook/maskformer-swin-base-ade",
                                                          id2label=id2label,
                                                          ignore_mismatched_sizes=True).to(device)

"""# Test the model"""

!pip install -q evaluate

import evaluate

metric = evaluate.load("mean_iou")

import torch
from tqdm.auto import tqdm

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=5e-6, weight_decay=1e-2)

running_loss = 0.0
num_samples = 0
for epoch in range(10):
  print("Epoch:", epoch)
  model.train()
  for idx, batch in enumerate(tqdm(train_dataloader)):
      #print(idx)
      # Reset the parameter gradients
      optimizer.zero_grad()

      # Forward pass
      outputs = model(
          pixel_values=batch["pixel_values"].to(device),
          mask_labels=[labels.to(device) for labels in batch["mask_labels"]],
          class_labels=[labels.to(device) for labels in batch["class_labels"]],
      )

      # Backward propagation
      loss = outputs.loss
      loss.backward()

      #print([labels.to(device) for labels in batch["class_labels"]])
      #print('_____')

      batch_size = batch["pixel_values"].size(0)
      running_loss += loss.item()
      num_samples += batch_size

      if idx % 100 == 0:
        print("Loss:", running_loss/num_samples)

      # Optimization
      optimizer.step()

batch_test = next((iter(test_dataloader)))

batch_test

import torch

# forward pass
with torch.no_grad():
  outputs = model(
      pixel_values=batch_test["pixel_values"].to(device),
      mask_labels=[labels.to(device) for labels in batch_test["mask_labels"]],
      class_labels=[labels.to(device) for labels in batch_test["class_labels"]],
  )

outputs['masks_queries_logits'].shape

from transformers import MaskFormerImageProcessor

processor = MaskFormerImageProcessor()

results = processor.post_process_instance_segmentation(outputs)[0]
print(results.keys())

results['segments_info']

for segment in results['segments_info']:
  print(segment)

import numpy as np

def get_mask(segmentation, segment_id):
  mask = (segmentation.cpu().numpy() == segment_id)
  visual_mask = (mask * 255).astype(np.uint8)
  visual_mask = Image.fromarray(visual_mask)

  return visual_mask

for segment in results['segments_info']:
    print("Visualizing mask for instance:", model.config.id2label[segment['label_id']])
    mask = get_mask(results['segmentation'], segment['id'])
    plt.imshow(mask)
    print("------")

